**本项目基于hugging face的transformer库，pytorch bert应用。**

写了个bert做文本分类的，写了调tiny_albert_chinese，bert_base_chinese, roberta_chinese等。  


可以很轻松跑起这个代码
1. 上hugging face下载对应bert的预训练文件
2. 数据集已上传，data目录下。
3. 打开config文件，将上述文件放到相应位置，修改config对应的路径、hidden_size参数
接下来就可以跑了。
>python train.py



## Albert Result:  
Epoch: 018; loss = 0.6691 cost time  16.4034   
Accuracy: 0.8015 Loss in test 0.6149  

              precision    recall  f1-score   support

          体育     0.7209    0.7180    0.7194      1000
          娱乐     0.8294    0.8020    0.8155      1000
          家居     0.6785    0.6520    0.6650      1000
          房产     0.9069    0.8860    0.8963      1000
          教育     0.7297    0.6910    0.7098      1000
          时尚     0.8275    0.8440    0.8356      1000
          时政     0.7438    0.8070    0.7741      1000
          游戏     0.9175    0.9230    0.9202      1000
          科技     0.8523    0.8370    0.8446      1000
          财经     0.8074    0.8550    0.8305      1000

        accuracy                         0.8015     10000
       macro avg     0.8014    0.8015    0.8011     10000
    weighted avg     0.8014    0.8015    0.8011     10000

## bert Result: 
Epoch: 010; loss = 0.2901 cost time  139.7863
Accuracy: 0.9156 Loss in test 0.2591
              precision    recall  f1-score   support

          体育     0.9266    0.8710    0.8979      1000
          娱乐     0.9235    0.9420    0.9327      1000
          家居     0.8458    0.8500    0.8479      1000
          房产     0.9637    0.9550    0.9593      1000
          教育     0.8634    0.8720    0.8677      1000
          时尚     0.9015    0.9240    0.9126      1000
          时政     0.9052    0.9070    0.9061      1000
          游戏     0.9816    0.9610    0.9712      1000
          科技     0.9450    0.9270    0.9359      1000
          财经     0.9045    0.9470    0.9253      1000

    accuracy                         0.9156     10000
       macro avg     0.9161    0.9156    0.9157     10000
    weighted avg     0.9161    0.9156    0.9157     10000

## macbert Result: 
Epoch: 008; loss = 0.3178 cost time  137.4945  
Accuracy: 0.9156 Loss in test 0.2811  

              precision    recall  f1-score   support

          体育     0.9203    0.8770    0.8981      1000
          娱乐     0.9529    0.9100    0.9309      1000
          家居     0.8299    0.8880    0.8580      1000
          房产     0.9174    0.9770    0.9462      1000
          教育     0.8645    0.8740    0.8692      1000
          时尚     0.9002    0.9380    0.9187      1000
          时政     0.9345    0.8700    0.9011      1000
          游戏     0.9711    0.9730    0.9720      1000
          科技     0.9485    0.9210    0.9346      1000
          财经     0.9280    0.9280    0.9280      1000

        accuracy                         0.9156     10000
       macro avg     0.9167    0.9156    0.9157     10000
    weighted avg     0.9167    0.9156    0.9157     10000


## roberta Result: 
Epoch: 009; loss = 0.3444 cost time  138.6462  
Accuracy: 0.9072 Loss in test 0.3152  

              precision    recall  f1-score   support

          体育     0.8837    0.8970    0.8903      1000
          娱乐     0.9718    0.8610    0.9130      1000
          家居     0.8473    0.8380    0.8426      1000
          房产     0.9348    0.9600    0.9472      1000
          教育     0.8885    0.8450    0.8662      1000
          时尚     0.8587    0.9540    0.9038      1000
          时政     0.9346    0.8720    0.9022      1000
          游戏     0.9905    0.9370    0.9630      1000
          科技     0.9417    0.9370    0.9393      1000
          财经     0.8466    0.9710    0.9045      1000

        accuracy                         0.9072     10000
       macro avg     0.9098    0.9072    0.9072     10000
    weighted avg     0.9098    0.9072    0.9072     10000

