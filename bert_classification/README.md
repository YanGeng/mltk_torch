**站在hugging face的transformer库肩膀上，pytorch花式调bert。**

写了个bert做文本分类的，写了调tiny_albert_chinese，bert_base_chinese的两种，大同小异，很容易上手。  


可以很轻松跑起这个代码
1. 上hugging face下载对应bert的预训练文件
2. 下载数据集 THUCNews（http://thuctc.thunlp.org/） 在这里，只切出了一部分数据做试验（没用完整数据集，完整数据集太大）。  
用作试验的部分数据在这里（https://pan.baidu.com/s/1J28HP5Sf32V4n19PRHaV5A  提取码：ddup）  
3. 打开config文件，将上述文件放到相应位置
接下来就可以跑了。
>python train.py  


显卡：一块1050Ti  
数据：10个类  
{'体育': 0, '娱乐': 1, '家居': 2, '房产': 3, '教育': 4, '时尚': 5, '时政': 6, '游戏': 7, '科技': 8, '财经': 9}  


Albert Result:  
16个epoch valid data result: Accuracy, 0.8400 Loss in test 0.4968  

classification result report: 

      class | precision | recall | f1-score | support  

      体育     0.9492    0.9720    0.9605       500
      娱乐     0.9431    0.8280    0.8818       500
      家居     0.7912    0.4320    0.5589       500
      房产     0.6824    0.8120    0.7416       500
      教育     0.8541    0.7140    0.7778       500
      时尚     0.8910    0.9320    0.9110       500
      时政     0.8802    0.8820    0.8811       500
      游戏     0.7636    0.9240    0.8362       500
      科技     0.8105    0.9580    0.8781       500
      财经     0.8711    0.9460    0.9070       500

    accuracy                         0.8400      5000
    macro avg     0.8436    0.8400    0.8334      5000
    weighted avg     0.8436    0.8400    0.8334      5000
    
    分类结果预测分布矩阵：
    [[486   0   0   0   3   0   1   3   7   0]
     [ 19 414   3   0   6   9   1  29  17   2]
     [  0   8 216 141  14  23  15  12  44  27]
     [  1   4  16 406  14   6  12   4   5  32]
     [  3   2  10   2 357   2  24  79  21   0]
     [  1   7  11   1   3 466   0   7   3   1]
     [  0   3   0  23  14   0 441   3   8   8]
     [  1   1   9   1   5  12   3 462   6   0]
     [  0   0   8   2   0   5   0   6 479   0]
     [  1   0   0  19   2   0   4   0   1 473]]
     

Bert-Base 卡不行，只调通代码，太慢没跑结果。 跑起来，跟上面步骤一样，换一下bert预训练文件就行

